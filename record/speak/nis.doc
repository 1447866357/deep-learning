natural image statistics
s作为对象或者world（？）特性或者特征，例如一张图片里面，距离等。生成模型合包括了si的先验概率，
（为什么已知si的ps）因为是先验概率，它指定了不同si组合同时出现的概率，在较为合理的情况下能够近似si的真实分布。
我们需要求解的是最大化p（s|I）的s，也就是数据由一些参数组成的统计模型描述，基于输入找到最好的参数。
通过先验概率ps与I和s之间的确定关系，

image space就是将像素点映射到3d空间中，下面两个坐标表示pixel的位置，z坐标是取值大小。 每个像素可用一个bit表示，可认为是编码，

可以认为是已知假设的情况下，求出从假设推出数据的概率，在实际的机器学习过程中，往往加入了很多的假设，假设数据的先验概率，也许是通过经验或者其他方法给出，例如ica中的cosh函数。

   像之前说过的ica是将独立分量分离出来，一张图片相当于一些独立的图片成分构成，也就是si。一般的ica考虑的就是从输入分离，而不像书中需要预处理（为什么？），预处理的理由是将输入降维舍弃携带较少信息量的维度，ica中需要使w协方差矩阵为单位矩阵希望学习到的基W的行向量不仅要线性独立，而且还是一组标准正交基。然后将原始数据 x 映射到特征si，我们已知ica是要求w的行向量具有正交性，ica的标准优化方式是使用投影梯度下降，假定每个si都有概率密度ps，求得px，由于不知道si的真实分布，假设为cosh或者sigmoid

TICA，拓扑的意思是视觉系统的细胞不是随机排列而是特殊的空间结构，神经元的回应也是系统性的变化。

1.已学东西的内容及应用
2.接下来的内容安排及流程规划
3.每个东西为什么这么做？应该出现什么样的结果？优缺点？

机器学习有两个主要方向，一个是回归，模拟输入分布，一个是分类问题，对输入进行分类

1.mlp，rbm，ica，rica
mlp通过输入乘以两层的权重和激励函数得到输出，再由输出与预期值计算出差错函数，这个差错函数就是跟权重相关，目标就是最小化差错函数，E（w）又是w的连续函数，当E（w）对w的偏导（也就是E的梯度）为0的时候，取值就是最小的，沿着梯度相反的方向下降最快，用梯度下降法更新w即相当于求到了下一个点，而这下一个点就是梯度更小的点，所以可以不断迭代来求得使梯度为0的点，这就是更新权重的过程，也就是求到最优的神经网络。激励函数就是似然函数，求它的负对数就得到差错函数（规定如此？）。

为什么使用mlp？
更加普遍的名称是bp神经网络 ，它是现在最常见常用的监督学习算法，学习参数，进行分类，  
缺点：需要大量的训练集，需要从头开始训练
应用：手写体识别，识别正常邮件，垃圾邮件，入侵检测中的系统的的正常访问跟非法访问。学习到的特征是针对每个输入进行提取，得到的更加具有针对性，分析图片

rbm 给出了可视节点和隐藏节点之间的联合能量，也就得出了联合概率密度，也就是最大化这个联合概率，求最大似然函数，也是梯度下降的方式，因为我们需要最大程度的拟合输入数据的分布，基于能量的模型能够很好的求得各种概率分布，模拟输入分布，隐藏节点可视为编码后的。
通过学习多层特征，最后学习到系统认为的输出或者分类，合成图片，一层波尔兹曼机学习一层特征，下一层学习上一层的特征再提取出更复杂的特征，再与bp结合组成一个完整的深度学习系统。
部分应用：中文文档分类，人脸识别，

//rbm和bp的结合构成了dbn。                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                     


ica主要用途是将独立量分离出来，例如在一张图片中（例子），可能就是图片中的某一部分，我们需要假设这些独立分量的分布，然后将这些分量给还原出来，就是说虽然不可能完全求出分量的分布，但是能够求出每个部分的分量，
应用：提取人脸表情，分离声音信号



2.内容安排
学习过程：共九层，每三层为一个整体，第一二层与卷积神经网络类似（参考论文le2010），第三层局部对比度归一化（参考3篇），重复三次。

训练集：10million 个200*200 的pixel图片，从10million的youtube视频里面每个截一张。    
image size在图中是表示成了200，但是实际操作的还是200*200，channel表示同时有三个输入图片生成不同的feature maps，18*18就是对应5*5，pooling size池化尺寸也就是对应2*2，每一层是一个map，3到8不清楚

L2 pooling
LCN（参考jarrett2009）
参数W1，W2，H大小
各个层的具体含义及个数含义
每层如何连接起来
tica
优化过程：如何通过RICA改变参数



结果：
1.测试人脸
37，000个图片来自两个数据集，13，026个是第一个数据集的全为脸，其他是随机图片例如自然风景等。

